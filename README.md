# ğŸ“„ VisualDocRAG â€” Retrieval-Augmented Generation for Visual Documents - Check at(https://visualdocrag.streamlit.app/)

VisualDocRAG is a Retrieval-Augmented Generation (RAG) pipeline that processes **PDFs, scanned images, and charts**, extracts text via OCR or native text extraction, and enables question answering using a vector database.

This project was built to demonstrate **document ingestion, semantic search, and LLM-based answering**.

---

## âœ¨ Features
- ğŸ“‚ **Multi-format ingestion** â€” PDF (native & scanned), JPG, PNG
- ğŸ” **Text + OCR extraction** (PyMuPDF & Tesseract)
- âœ‚ **Chunking & metadata tagging**
- ğŸ“¦ **Vector store with ChromaDB** (persistent storage)
- ğŸ¤– **LLM-based answer generation** (OpenRouter / OpenAI API)
- ğŸ–¥ **Streamlit-based UI**
- ğŸ“Š **Evaluation script** with latency metrics and RAGAS-ready JSON

---

## ğŸ“¦ Requirements

- Python 3.10+
- [Tesseract OCR](https://github.com/UB-Mannheim/tesseract/wiki) installed & added to PATH (required for scanned docs)
- An **OpenRouter** or **OpenAI API key** for abstractive answers

---

## âš™ Installation

```bash
# 1. Clone the repository
git clone https://github.com/ss.shivam1406/visual-doc-rag.git
cd visual-doc-rag

# 2. Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate   # Mac/Linux

# 3. Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“„ Environment Variables
### Create a ```.env``` file in the root:
```bash
# API keys (use one of these)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

OPENROUTER_API_KEY=sk-or-...
OPENROUTER_MODEL=openrouter/your-model
```

---

## ğŸš€ Running the App
```bash
streamlit run app.py
```
Then:
- Upload PDFs/images via the Ingest File section.
- Run queries in the Ask a Question section.

---

## ğŸ“Š Running Evaluation
We include ```evaluate.py``` to run automated queries and export RAGAS-ready results:
```bash
python evaluate.py
```

- If ```.chroma/``` already exists, it will use the same ingested data from the frontend.
- If not, you can modify ```evaluate.py``` to auto-ingest test data before evaluation.

Output:
- ```evaluation_results.json``` â†’ contains query, predicted answer, gold answer, contexts, and latency.

---

## ğŸ§± Project Structure
```
visual-doc-rag/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ evaluate.py            # Evaluation script
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ .env                   # Environment variables
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ config.py          # Settings loader
    â”œâ”€â”€ pipeline.py        # Main VisualDocRAG orchestrator
    â”œâ”€â”€ loaders.py         # PDF/image loaders (OCR & text)
    â”œâ”€â”€ chunking.py        # Chunking logic
    â”œâ”€â”€ vectorstore.py     # ChromaDB vector store
    â”œâ”€â”€ retriever.py       # Top-K semantic retriever
    â”œâ”€â”€ generator.py       # Answer generation (OpenRouter OpenAI/local)
    â””â”€â”€ utils.py           # Helper functions
```

---

## ğŸ›  Troubleshooting

-**Missing imports in VS Code**: Make sure you select the ```.venv``` interpreter and install all dependencies.

-**TesseractNotFoundError**: Install Tesseract OCR and add it to PATH.

-**Quota errors**: Switch from OpenAI API to OpenRouter or update your billing plan.

-**Evaluation returns no answers**: Ensure ```.chroma/``` exists (ingest docs first).

---

## ğŸ¤ Contribute
Pull requests welcome! For major changes, open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“„ License
This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™‹â€â™‚ï¸ Author

**Shivam**  
ğŸ“§ [ss.shivam1406@gmail.com](mailto:ss.shivam1406@gmail.com)  
ğŸ”— [LinkedIn â€“ ssshivam1406](https://www.linkedin.com/in/ssshivam1406/)
